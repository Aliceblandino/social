{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3e39bb",
   "metadata": {},
   "source": [
    "# Progetto di social computing 2025\n",
    "### Analisi di dataset di scopus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056f6a1",
   "metadata": {},
   "source": [
    "## Analisi dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d248189b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/nuovo_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Carica dataset\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/nuovo_dataset.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Rimuovi righe senza autori o anno\u001b[39;00m\n\u001b[32m     13\u001b[39m df = df.dropna(subset=[\u001b[33m'\u001b[39m\u001b[33mAuthor full names\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elimo\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elimo\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elimo\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elimo\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elimo\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/nuovo_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "from itertools import combinations\n",
    "\n",
    "# =========================\n",
    "# Carica dataset\n",
    "# =========================\n",
    "df = pd.read_csv(\"../data/nuovo_dataset.csv\")\n",
    "\n",
    "# Rimuovi righe senza autori o anno\n",
    "df = df.dropna(subset=['Author full names', 'Year'])\n",
    "\n",
    "# =========================\n",
    "# Crea grafi per anno\n",
    "# =========================\n",
    "grafi_per_anno = {}\n",
    "\n",
    "for anno in df['Year'].unique():\n",
    "    G = nx.Graph()\n",
    "    df_anno = df[df['Year'] == anno]\n",
    "\n",
    "    for authors_str in df_anno['Author full names']:\n",
    "        authors = [a.strip() for a in authors_str.split(';')]\n",
    "        # crea arco tra ogni coppia di autori\n",
    "        for u, v in combinations(authors, 2):\n",
    "            if G.has_edge(u, v):\n",
    "                G[u][v]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(u, v, weight=1)\n",
    "    grafi_per_anno[anno] = G\n",
    "\n",
    "print(\"Grafi creati per anno:\", list(grafi_per_anno.keys()))\n",
    "\n",
    "# =========================\n",
    "# Dizionari per risultati\n",
    "# =========================\n",
    "central_node_per_anno = {}\n",
    "highlight_nodes_per_anno = {}\n",
    "subgrafi_per_anno = {}\n",
    "pos_per_anno = {}\n",
    "\n",
    "# =========================\n",
    "# Nodo centrale, sottografi e serializzazione JSON\n",
    "# =========================\n",
    "for anno, G in grafi_per_anno.items():\n",
    "    if len(G) == 0:\n",
    "        continue\n",
    "\n",
    "    # Nodo centrale (grado massimo)\n",
    "    central = max(G.degree, key=lambda x: x[1])[0]\n",
    "    central_node_per_anno[anno] = central\n",
    "\n",
    "    # Vicini equicentrali\n",
    "    central_degree = G.degree(central)\n",
    "    equicentral_neighbors = [\n",
    "        n for n in G.neighbors(central)\n",
    "        if G.degree(n) == central_degree\n",
    "    ]\n",
    "    highlight_nodes_per_anno[anno] = [central] + equicentral_neighbors\n",
    "\n",
    "    # Sottografo centrale + vicini\n",
    "    neighbors = list(G.neighbors(central))\n",
    "    nodes = [central] + neighbors\n",
    "    H = G.subgraph(nodes).copy()\n",
    "    subgrafi_per_anno[anno] = H\n",
    "\n",
    "    # Posizioni\n",
    "    initial_pos = {central: (0, 0)}\n",
    "    pos = nx.spring_layout(H, seed=42, pos=initial_pos, fixed=[central])\n",
    "    pos_per_anno[anno] = pos\n",
    "\n",
    "    # Serializzazione JSON\n",
    "    data_json = json_graph.node_link_data(H)\n",
    "    filename = f\"../graphs/subgrafo_{anno}.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Serializzazione dei sottografi completata!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aafece0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Authors', 'Author full names', 'Author(s) ID', 'Title', 'Year',\n",
       "       'Source title', 'Volume', 'Cited by', 'DOI', 'Link', 'Affiliations',\n",
       "       'Authors with affiliations', 'Abstract', 'Author Keywords',\n",
       "       'Index Keywords', 'References', 'Correspondence Address', 'Publisher',\n",
       "       'ISSN', 'Abbreviated Source Title', 'Document Type',\n",
       "       'Publication Stage', 'EID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutti i sottografi salvati in un unico file: grafi_tutti.json\n"
     ]
    }
   ],
   "source": [
    "grafi_unico_json = {}\n",
    "\n",
    "for anno in df['Year'].unique():\n",
    "    G = nx.Graph()\n",
    "    df_anno = df[df['Year'] == anno]\n",
    "\n",
    "    for authors_str in df_anno['Author full names']:\n",
    "        authors = [a.strip() for a in authors_str.split(';')]\n",
    "        for u, v in combinations(authors, 2):\n",
    "            if G.has_edge(u, v):\n",
    "                G[u][v]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(u, v, weight=1)\n",
    "\n",
    "    if len(G) > 0:\n",
    "        central = max(G.degree, key=lambda x: x[1])[0]\n",
    "        central_degree = G.degree(central)\n",
    "        equicentral_neighbors = [\n",
    "            n for n in G.neighbors(central)\n",
    "            if G.degree(n) == central_degree\n",
    "        ]\n",
    "        nodes = [central] + equicentral_neighbors\n",
    "        H = G.subgraph(nodes).copy()\n",
    "    else:\n",
    "        H = G\n",
    "\n",
    "    # ✅ Chiave convertita in stringa\n",
    "    grafi_unico_json[str(anno)] = json_graph.node_link_data(H)\n",
    "\n",
    "# Salva in un unico JSON\n",
    "with open(\"../graphs/grafi_tutti.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grafi_unico_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Tutti i sottografi salvati in un unico file: grafi_tutti.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf94304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gigli, Gian Luigi (7101781977)'] []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "import networkx as nx\n",
    "\n",
    "# Carica JSON\n",
    "with open(\"../graphs/grafi_tutti.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dati = json.load(f)\n",
    "\n",
    "# Esempio: ricostruire il grafo del 2020\n",
    "G2020 = json_graph.node_link_graph(dati[\"2020\"])\n",
    "print(G2020.nodes(), G2020.edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7cd59",
   "metadata": {},
   "source": [
    "# INIZIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b49181",
   "metadata": {},
   "source": [
    "LISTA AUTORI BELLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd2518",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mauthors_list\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mAuthor full names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m     .str.split(\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     .apply(\u001b[38;5;28;01mlambda\u001b[39;00m authors: [\n\u001b[32m      5\u001b[39m         re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m([^)]*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, a)  \u001b[38;5;66;03m# rimuove (ID)\u001b[39;00m\n\u001b[32m      6\u001b[39m         .replace(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)               \u001b[38;5;66;03m# rimuove virgole\u001b[39;00m\n\u001b[32m      7\u001b[39m         .strip()\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m authors\n\u001b[32m      9\u001b[39m     ])\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mauthors_list\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"authors_list\"] = (\n",
    "    df[\"Author full names\"]\n",
    "    .str.split(\";\")\n",
    "    .apply(lambda authors: [\n",
    "        re.sub(r\"\\s*\\([^)]*\\)\", \"\", a)  # rimuove (ID)\n",
    "        .replace(\",\", \"\")               # rimuove virgole\n",
    "        .strip()\n",
    "        for a in authors\n",
    "    ])\n",
    ")\n",
    "df[\"authors_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c17ff9",
   "metadata": {},
   "source": [
    "Grafi per anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdc5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "grafi_per_anno = {}\n",
    "for anno, df_anno in df.groupby(\"Year\"):\n",
    "    G = nx.Graph()\n",
    "    for authors in df_anno[\"authors_list\"].dropna():\n",
    "        for a1, a2 in itertools.combinations(authors, 2):\n",
    "            if G.has_edge(a1, a2):\n",
    "                G[a1][a2][\"weight\"] += 1\n",
    "            else:\n",
    "                G.add_edge(a1, a2, weight=1)\n",
    "    grafi_per_anno[anno] = G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dcf6dc",
   "metadata": {},
   "source": [
    "# GRAFI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b95a76",
   "metadata": {},
   "source": [
    "GRAFO 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde5ca13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grafi_per_anno' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Nodo centrale per anno\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      4\u001b[39m central_node_per_anno = {}\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m anno, G \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgrafi_per_anno\u001b[49m.items():\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(G) == \u001b[32m0\u001b[39m:\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'grafi_per_anno' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Nodo centrale per anno\n",
    "# =========================\n",
    "central_node_per_anno = {}\n",
    "for anno, G in grafi_per_anno.items():\n",
    "    if len(G) == 0:\n",
    "        continue\n",
    "    central_node_per_anno[anno] = max(G.degree, key=lambda x: x[1])[0]\n",
    "\n",
    "# =========================\n",
    "# Vicini equicentrali del nodo centrale\n",
    "# =========================\n",
    "highlight_nodes_per_anno = {}\n",
    "for anno, G in grafi_per_anno.items():\n",
    "    if anno not in central_node_per_anno:\n",
    "        continue\n",
    "    central = central_node_per_anno[anno]\n",
    "    central_degree = G.degree(central)\n",
    "    equicentral_neighbors = [\n",
    "        n for n in G.neighbors(central)\n",
    "        if G.degree(n) == central_degree\n",
    "    ]\n",
    "    highlight_nodes_per_anno[anno] = [central] + equicentral_neighbors\n",
    "\n",
    "# =========================\n",
    "# Sottografo: centrale + vicini\n",
    "# =========================\n",
    "subgrafi_per_anno = {}\n",
    "pos_per_anno = {}\n",
    "\n",
    "for anno, G in grafi_per_anno.items():\n",
    "    if anno not in central_node_per_anno:\n",
    "        continue\n",
    "    central = central_node_per_anno[anno]\n",
    "    neighbors = list(G.neighbors(central))\n",
    "    nodes = [central] + neighbors\n",
    "    H = G.subgraph(nodes).copy()\n",
    "    subgrafi_per_anno[anno] = H\n",
    "\n",
    "    initial_pos = {central: (0, 0)}\n",
    "    pos = nx.spring_layout(H, seed=42, pos=initial_pos, fixed=[central])\n",
    "    pos_per_anno[anno] = pos\n",
    "\n",
    "# =========================\n",
    "# Funzione Plotly\n",
    "# =========================\n",
    "def plot_graph_plotly(G, pos, highlight_nodes):\n",
    "    edge_x, edge_y = [], []\n",
    "    for u, v in G.edges():\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        mode=\"lines\",\n",
    "        line=dict(width=1, color=\"gray\"),\n",
    "        hoverinfo=\"none\",\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    node_x, node_y, texts, colors, sizes = [], [], [], [], []\n",
    "\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        texts.append(f\"{node}<br>Grado: {G.degree(node)}\")\n",
    "\n",
    "        if node in highlight_nodes:\n",
    "            colors.append(\"crimson\")\n",
    "            sizes.append(11)\n",
    "        else:\n",
    "            colors.append(\"royalblue\")\n",
    "            sizes.append(8)\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode=\"markers\",\n",
    "        text=texts,\n",
    "        hoverinfo=\"text\",\n",
    "        marker=dict(\n",
    "            size=sizes,\n",
    "            color=colors,\n",
    "            line=dict(width=0.5, color=\"black\")\n",
    "        ),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    return edge_trace, node_trace\n",
    "\n",
    "# =========================\n",
    "# Genera titoli subplot\n",
    "# =========================\n",
    "authors_per_anno = {}\n",
    "for anno, G in grafi_per_anno.items():\n",
    "    if anno not in central_node_per_anno:\n",
    "        continue\n",
    "    central = central_node_per_anno[anno]\n",
    "    central_degree = G.degree(central)\n",
    "    equicentral_neighbors = [\n",
    "        n for n in G.neighbors(central)\n",
    "        if G.degree(n) == central_degree\n",
    "    ]\n",
    "    all_authors = [central] + equicentral_neighbors\n",
    "    authors_per_anno[anno] = all_authors\n",
    "\n",
    "\n",
    "anni = sorted(grafi_per_anno.keys())\n",
    "\n",
    "\n",
    "titoli = []\n",
    "for anno in anni:\n",
    "    authors = authors_per_anno.get(anno, [])\n",
    "    # massimo 3 autori\n",
    "    if len(authors) > 3:\n",
    "        authors = authors[:3] + [\"...\"]\n",
    "    # raggruppa ogni 2 autori per riga\n",
    "    lines = []\n",
    "    for i in range(0, len(authors), 2):\n",
    "        lines.append(\", \".join(authors[i:i+2]))\n",
    "    titolo = f\"{anno}<br>\" + \"<br>\".join(lines)\n",
    "    titoli.append(titolo)\n",
    "\n",
    "# =========================\n",
    "# Crea figura 2x5\n",
    "# =========================\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=5,\n",
    "    subplot_titles=titoli,\n",
    "    horizontal_spacing=0.03,\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "for i, anno in enumerate(anni):\n",
    "    if anno not in subgrafi_per_anno:\n",
    "        continue\n",
    "    G = subgrafi_per_anno[anno]\n",
    "    pos = pos_per_anno[anno]\n",
    "    highlight_nodes = highlight_nodes_per_anno[anno]\n",
    "\n",
    "    edge, node = plot_graph_plotly(G, pos, highlight_nodes)\n",
    "\n",
    "    row = 1 if i < 5 else 2\n",
    "    col = i + 1 if i < 5 else i - 4\n",
    "\n",
    "    fig.add_trace(edge, row=row, col=col)\n",
    "    fig.add_trace(node, row=row, col=col)\n",
    "\n",
    "# =========================\n",
    "# Layout finale\n",
    "# =========================\n",
    "fig.update_layout(\n",
    "    height=650,\n",
    "    width=1400,\n",
    "    paper_bgcolor=\"white\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    title_x=0.5,\n",
    "    font=dict(size=10),\n",
    "    margin=dict(l=20, r=20, t=50, b=20)\n",
    ")\n",
    "\n",
    "for r in [1, 2]:\n",
    "    for c in range(1, 6):\n",
    "        fig.update_xaxes(visible=False, row=r, col=c)  \n",
    "        fig.update_yaxes(visible=False, row=r, col=c)  \n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(t=50, b=20, l=20, r=20),\n",
    ")\n",
    "\n",
    "for ann in fig.layout.annotations:\n",
    "    ann.font = dict(size=12)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f87b266",
   "metadata": {},
   "source": [
    "GRAFO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433bdcc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grafi_per_anno' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Grafo totale e autore centrale globale\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m G_totale = nx.compose_all(\u001b[43mgrafi_per_anno\u001b[49m.values())\n\u001b[32m      5\u001b[39m central_author = \u001b[38;5;28mmax\u001b[39m(G_totale.degree, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m])[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAutore centrale globale:\u001b[39m\u001b[33m\"\u001b[39m, central_author)\n",
      "\u001b[31mNameError\u001b[39m: name 'grafi_per_anno' is not defined"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Grafo totale e autore centrale globale\n",
    "# ==============================\n",
    "G_totale = nx.compose_all(grafi_per_anno.values())\n",
    "central_author = max(G_totale.degree, key=lambda x: x[1])[0]\n",
    "print(\"Autore centrale globale:\", central_author)\n",
    "\n",
    "# ==============================\n",
    "# Creazione grafi cumulativi\n",
    "# ==============================\n",
    "grafi_cumulativi = {}\n",
    "G_cumulativo = nx.Graph()\n",
    "\n",
    "for anno in sorted(grafi_per_anno.keys()):\n",
    "    G_cumulativo = nx.compose(G_cumulativo, grafi_per_anno[anno])\n",
    "    grafi_cumulativi[anno] = G_cumulativo.copy()\n",
    "\n",
    "# ==============================\n",
    "# Calcolo layout per anno\n",
    "# ==============================\n",
    "pos_per_anno = {}\n",
    "for anno, G in grafi_cumulativi.items():\n",
    "    if central_author not in G:\n",
    "        continue\n",
    "\n",
    "    nodes_connected = nx.node_connected_component(G, central_author)\n",
    "    G_sub = G.subgraph(nodes_connected).copy()\n",
    "\n",
    "    initial_pos = {central_author: (0, 0)}\n",
    "\n",
    "    pos = nx.spring_layout(\n",
    "        G_sub,\n",
    "        seed=42,\n",
    "        pos=initial_pos,\n",
    "        fixed=[central_author],\n",
    "        k=0.5,\n",
    "        iterations=200\n",
    "    )\n",
    "\n",
    "    xs = [x for x, y in pos.values()]\n",
    "    ys = [y for x, y in pos.values()]\n",
    "    min_x, max_x = min(xs), max(xs)\n",
    "    min_y, max_y = min(ys), max(ys)\n",
    "    for node in pos:\n",
    "        x, y = pos[node]\n",
    "        norm_x = -1.1 + 2.2 * (x - min_x) / (max_x - min_x) if max_x > min_x else 0\n",
    "        norm_y = -1.1 + 2.2 * (y - min_y) / (max_y - min_y) if max_y > min_y else 0\n",
    "        pos[node] = (norm_x, norm_y)\n",
    "\n",
    "    pos_per_anno[anno] = pos\n",
    "    grafi_cumulativi[anno] = G_sub  \n",
    "\n",
    "# ==============================\n",
    "# Funzione di plotting\n",
    "# ==============================\n",
    "def plot_graph_plotly(G, pos, central_node):\n",
    "    edge_x, edge_y = [], []\n",
    "\n",
    "    for u, v in G.edges():\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        mode=\"lines\",\n",
    "        line=dict(width=1, color=\"gray\"),\n",
    "        hoverinfo=\"none\",\n",
    "        name=\"Collaborazioni\",\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    node_x, node_y, texts, colors, sizes = [], [], [], [], []\n",
    "\n",
    "    for node in G.nodes():\n",
    "        if node == central_node:\n",
    "            continue\n",
    "        x, y = pos[node]\n",
    "        deg = G.degree(node)\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        texts.append(f\"{node}<br>Collaborazioni: {deg}\")\n",
    "        colors.append(\"royalblue\")\n",
    "        sizes.append(8)\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode=\"markers\",\n",
    "        hoverinfo=\"text\",\n",
    "        text=texts,\n",
    "        marker=dict(\n",
    "            size=sizes,\n",
    "            color=colors,\n",
    "            line=dict(width=0.5, color=\"black\"),\n",
    "            opacity=1.0\n",
    "        ),\n",
    "        name=\"Autori\"\n",
    "    )\n",
    "\n",
    "    x_c, y_c = pos[central_node]\n",
    "    node_central_trace = go.Scatter(\n",
    "        x=[x_c],\n",
    "        y=[y_c],\n",
    "        mode=\"markers\",\n",
    "        hoverinfo=\"text\",\n",
    "        text=[f\"{central_node}<br>Collaborazioni: {G.degree(central_node)}\"],\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color=\"crimson\",\n",
    "            line=dict(width=0.5, color=\"black\"),\n",
    "            opacity=1.0\n",
    "        ),\n",
    "        name=\"Gigli G.L.\"\n",
    "    )\n",
    "\n",
    "    return edge_trace, node_trace, node_central_trace\n",
    "\n",
    "# ==============================\n",
    "# Creazione dei frame\n",
    "# ==============================\n",
    "frames = []\n",
    "\n",
    "for anno in sorted(grafi_cumulativi.keys()):\n",
    "    G = grafi_cumulativi[anno]\n",
    "    if anno not in pos_per_anno: \n",
    "        continue\n",
    "    pos = pos_per_anno[anno]\n",
    "    edge, nodes, central = plot_graph_plotly(G, pos, central_author)\n",
    "    frames.append(\n",
    "        go.Frame(\n",
    "            data=[edge, nodes, central],\n",
    "            name=str(anno),\n",
    "            layout=go.Layout(\n",
    "                title=f\"Collaborazioni cumulative fino al {anno}<br>Autore centrale: {central_author}\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Figura finale\n",
    "# ==============================\n",
    "fig = go.Figure(\n",
    "    data=frames[0].data,\n",
    "    frames=frames,\n",
    "    layout=go.Layout(\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\",\n",
    "        xaxis=dict(visible=False, range=[-1.5,1.5]),\n",
    "        yaxis=dict(visible=False, range=[-1.5,1.5]),\n",
    "        updatemenus=[{\n",
    "            \"type\": \"buttons\",\n",
    "            \"buttons\": [{\n",
    "                \"label\": \"Play\",\n",
    "                \"method\": \"animate\",\n",
    "                \"args\": [None, {\n",
    "                    \"frame\": {\"duration\":2000, \"redraw\":True},\n",
    "                    \"transition\":{\"duration\":500},\n",
    "                    \"fromcurrent\":True\n",
    "                }]\n",
    "            }]\n",
    "        }],\n",
    "        sliders=[{\n",
    "            \"steps\": [{\"method\":\"animate\", \"args\":[[f.name]], \"label\":f.name} for f in frames]\n",
    "        }]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a60762",
   "metadata": {},
   "source": [
    "GRAFO 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ed34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo generato con successo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "\n",
    "def genera_grafo_statico(file_csv):\n",
    "    try:\n",
    "        # Caricamento del dataset fornito\n",
    "        df = pd.read_csv(file_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel caricamento del file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calcolo frequenze e impatto citazioni dal dataset\n",
    "    counts = df['Source title'].value_counts().to_dict()\n",
    "    impact = df.groupby('Source title')['Cited by'].sum().to_dict() if 'Cited by' in df.columns else counts\n",
    "\n",
    "    net = Network(height=\"900px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\", select_menu=False)\n",
    "\n",
    "    # Dizionario tematico tradotto e ottimizzato\n",
    "    topic_mapping = {\n",
    "        \"Neuroscienze\": {\"color\": \"#9370db\", \"keywords\": [\"Brain\", \"Neuro\", \"Epileps\", \"Seizure\", \"Sleep\", \"Neurology\", \"Cerebellum\", \"Sclerosis\"]},\n",
    "        \"Informatica e IA\": {\"color\": \"#97c2fc\", \"keywords\": [\"Computer\", \"Computing\", \"Software\", \"Logic\", \"Programming\", \"ACM\", \"Intelligence\", \"Automated Planning\", \"Heuristics\"]},\n",
    "        \"Ingegneria e Segnali\": {\"color\": \"#40e0d0\", \"keywords\": [\"IEEE\", \"Signal\", \"Engineering\", \"Mechanical\", \"Electronics\", \"IMEKO\", \"Instruments\", \"Circuits\", \"Manufacturing\", \"Process Safety\"]},\n",
    "        \"Scienze della Vita e Bio\": {\"color\": \"#ff69b4\", \"keywords\": [\"Bioinformatics\", \"Molecular\", \"Genetics\", \"Cancers\", \"Molecules\", \"Bio\", \"Nanoscale\", \"Thrombosis\"]},\n",
    "        \"Matematica e Teoria\": {\"color\": \"#6a5acd\", \"keywords\": [\"Mathematics\", \"Matematica\", \"Algorithms\", \"Theory\", \"Calculus\", \"Formal\", \"Discrete\", \"Combinatorial\", \"Optimization\"]},\n",
    "        \"Fisica\": {\"color\": \"#7be141\", \"keywords\": [\"Physics\", \"Nuclear\", \"Physical Review\", \"Frontiers in Physics\"]},\n",
    "        \"Medicina Clinica\": {\"color\": \"#fb7e81\", \"keywords\": [\"Clinical\", \"Medicine\", \"Health\", \"Biomedica\", \"Acta Biomedica\", \"Expert Opinion\"]},\n",
    "        \"Sostenibilità e Scienze Sociali\": {\"color\": \"#20b2aa\", \"keywords\": [\"Sustainability\", \"Education\", \"Science as Culture\", \"Transportation\", \"Social-Informatics\"]},\n",
    "        \"Multidisciplinare\": {\"color\": \"#c0c0c0\", \"keywords\": [\"Scientific Reports\", \"Applied Sciences\", \"Nature\", \"MethodsX\", \"Proceedings of Science\"]}\n",
    "    }\n",
    "\n",
    "    # Aggiunta Macro-aree (Dimensioni aumentate a 80)\n",
    "    for topic, info in topic_mapping.items():\n",
    "        net.add_node(topic, label=topic, color=info[\"color\"], size=80, shape=\"ellipse\")\n",
    "\n",
    "    # Aggiunta Riviste dal dataset\n",
    "    for title, freq in counts.items():\n",
    "        if pd.isna(title): continue\n",
    "        \n",
    "        assigned_cats = []\n",
    "        for topic, info in topic_mapping.items():\n",
    "            for kw in info[\"keywords\"]:\n",
    "                if kw.lower() in str(title).lower():\n",
    "                    assigned_cats.append(topic)\n",
    "                    break\n",
    "        \n",
    "        if not assigned_cats: assigned_cats = [\"Multidisciplinare\"]\n",
    "\n",
    "        primary_col = topic_mapping[assigned_cats[0]][\"color\"]\n",
    "        imp_value = impact.get(title, 0)\n",
    "        node_size = 15 + (min(imp_value, 50) / 2) if imp_value > 0 else 15\n",
    "        \n",
    "        net.add_node(title, label=title, shape=\"box\", \n",
    "                     color={\"border\": primary_col, \"background\": \"#ffffff\"}, \n",
    "                     size=node_size)\n",
    "\n",
    "        for cat in assigned_cats:\n",
    "            net.add_edge(cat, title, value=freq, color=topic_mapping[cat][\"color\"])\n",
    "\n",
    "    # CONFIGURAZIONE FISICA\n",
    "    options = {\n",
    "      \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "          \"gravitationalConstant\": -150,\n",
    "          \"centralGravity\": 0.01,\n",
    "          \"springLength\": 200,\n",
    "          \"avoidOverlap\": 1\n",
    "        },\n",
    "        \"solver\": \"forceAtlas2Based\",\n",
    "        \"stabilization\": {\n",
    "          \"enabled\": True,\n",
    "          \"iterations\": 1000,\n",
    "          \"updateInterval\": 25,\n",
    "          \"fit\": True\n",
    "        }\n",
    "      },\n",
    "      \"nodes\": {\n",
    "        \"borderWidth\": 2\n",
    "      },\n",
    "      \"interaction\": {\n",
    "        \"dragNodes\": True,\n",
    "        \"hover\": True,\n",
    "        \"navigationButtons\": True\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    net.set_options(json.dumps(options))\n",
    "\n",
    "    # Salvataggio file\n",
    "    output_file = \"grafo_statico.html\"\n",
    "    net.save_graph(output_file)\n",
    "\n",
    "    # AGGIUNTA MANUALE DELLA LEGENDA HTML\n",
    "    legenda_div = \"\"\"\n",
    "    <div style=\"position: fixed; top: 10px; right: 10px; width: 200px; padding: 10px; background-color: rgba(255,255,255,0.8); border: 1px solid #ccc; border-radius: 8px; z-index: 1000; font-family: Arial, sans-serif;\">\n",
    "        <h4 style=\"margin: 0 0 10px 0; font-size: 14px;\">Legenda Aree</h4>\n",
    "    \"\"\"\n",
    "    for topic, info in topic_mapping.items():\n",
    "        legenda_div += f'<div style=\"display: flex; align-items: center; margin-bottom: 5px;\"><div style=\"width: 12px; height: 12px; background-color: {info[\"color\"]}; border-radius: 50%; margin-right: 8px;\"></div><span style=\"font-size: 12px;\">{topic}</span></div>'\n",
    "    legenda_div += \"</div>\"\n",
    "\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # Inseriamo la legenda dopo il tag body\n",
    "    html_content = html_content.replace('<body>', f'<body>{legenda_div}')\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(\"Grafo generato con successo\")\n",
    "\n",
    "# Esecuzione\n",
    "genera_grafo_statico(\"nuovo_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9ee58",
   "metadata": {},
   "source": [
    "Analisi collaborazioni Udine - Italia - Estero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a0f20de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICA TOP 5 COLLABORAZIONI TRA ITALIA ED ESTERO\n",
      "\n",
      "ITALIA:\n",
      "Affiliation                                                                                                weight\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Università degli Studi di Padova, Padua, PD, Italy                                                         8     \n",
      "Università degli Studi di Trieste, Trieste, TS, Italy                                                      5     \n",
      "Istituto Nazionale di Fisica Nucleare, Sezione di Trieste, Trieste, TS, Italy                              5     \n",
      "Istituto Nazionale di Biostrutture e Biosistemi, Rome, RM, Italy                                           3     \n",
      "Department of Medical Sciences and Public Health, Università degli Studi di Cagliari, Cagliari, CA, Italy  3     \n",
      "\n",
      "ESTERO:\n",
      "Affiliation                                                                                                                     weight\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Universitat Politècnica de València, Valencia, Valencia, Spain                                                                  9     \n",
      "Division of Science and Mathematics, NYU Abu Dhabi, Abu Dhabi, Abu Dhabi, United Arab Emirates                                  5     \n",
      "Institut NeuroMyoGène, Lyon, Auvergne-Rhone-Alpes, France                                                                       4     \n",
      "Université Claude Bernard Lyon 1, Villeurbanne, Auvergne-Rhone-Alpes, France                                                    4     \n",
      "Institute of AI for Health, Helmholtz Center Munich German Research Center for Environmental Health, Oberschleissheim, Germany  3     \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# =====================================\n",
    "# 1. CARICAMENTO E PULIZIA\n",
    "# =====================================\n",
    "df = pd.read_csv(\"nuovo_dataset.csv\")\n",
    "df = df.dropna(subset=[\"Affiliations\"])\n",
    "\n",
    "# 2. IDENTIFICAZIONE PAPER CON UDINE\n",
    "df_udine = df[df[\"Affiliations\"].str.contains(\"Udine\", case=False, na=False)].copy()\n",
    "\n",
    "# =====================================\n",
    "# 3. PROCEDIMENTO DI CALCOLO\n",
    "# =====================================\n",
    "italy_counter = Counter()\n",
    "foreign_counter = Counter()\n",
    "\n",
    "# Fase A: Trova i nomi dei partner\n",
    "for aff_string in df_udine[\"Affiliations\"]:\n",
    "    parts = [p.strip() for p in str(aff_string).split(\";\")]\n",
    "    for p in parts:\n",
    "        if \"udine\" in p.lower() or p == \"\": continue\n",
    "        if \"italy\" in p.lower():\n",
    "            italy_counter[p] += 1\n",
    "        else:\n",
    "            foreign_counter[p] += 1\n",
    "\n",
    "# Fase B: Calcola i pesi (weight)\n",
    "all_partners = list(italy_counter.keys()) + list(foreign_counter.keys())\n",
    "partner_data = []\n",
    "\n",
    "for p_name in all_partners:\n",
    "    count = 0\n",
    "    p_name_lower = p_name.lower()\n",
    "    for row_aff in df_udine[\"Affiliations\"]:\n",
    "        if p_name_lower in str(row_aff).lower():\n",
    "            count += 1\n",
    "    \n",
    "    if count >= 2: # Mantieni chi ha almeno 2 collaborazioni\n",
    "        p_type = \"Italian\" if \"italy\" in p_name.lower() else \"Foreign\"\n",
    "        partner_data.append({\"Affiliation\": p_name, \"Type\": p_type, \"weight\": count})\n",
    "\n",
    "collab_counts = pd.DataFrame(partner_data)\n",
    "\n",
    "# =====================================\n",
    "# 4. FUNZIONE DI STAMPA (PULITA)\n",
    "# =====================================\n",
    "def print_left_aligned(df, title):\n",
    "    print(f\"\\n{title}\")\n",
    "    if df.empty:\n",
    "        print(\"Nessun dato trovato.\")\n",
    "        return\n",
    "    df_str = df[['Affiliation', 'weight']].astype(str)\n",
    "    widths = [max(df_str[col].str.len().max(), len(col)) for col in df_str.columns]\n",
    "    header_row = \"  \".join([col.ljust(widths[i]) for i, col in enumerate(df_str.columns)])\n",
    "    print(header_row)\n",
    "    print(\"-\" * len(header_row))\n",
    "    for _, row in df_str.iterrows():\n",
    "        print(\"  \".join([row[col].ljust(widths[i]) for i, col in enumerate(df_str.columns)]))\n",
    "\n",
    "# =====================================\n",
    "# 5. OUTPUT FINALE (TOP 5)\n",
    "# =====================================\n",
    "top_italy = collab_counts[collab_counts[\"Type\"]==\"Italian\"].sort_values(by=\"weight\", ascending=False).head(5)\n",
    "top_foreign = collab_counts[collab_counts[\"Type\"]==\"Foreign\"].sort_values(by=\"weight\", ascending=False).head(5)\n",
    "\n",
    "print(\"CLASSIFICA TOP 5 COLLABORAZIONI TRA ITALIA ED ESTERO\")\n",
    "print_left_aligned(top_italy, \"ITALIA:\")\n",
    "print_left_aligned(top_foreign, \"ESTERO:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
